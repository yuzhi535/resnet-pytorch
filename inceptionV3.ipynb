{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inceptionV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+R1EQFi+IUXO9MY4J0tr3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuzhi535/resnet-pytorch/blob/master/inceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install timm torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8rAmjYjqrGF",
        "outputId": "4804e3c6-0417-441e-9533-7ee0f1e488c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.6.7)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 定义网络"
      ],
      "metadata": {
        "id": "2o_J9kKeqYg_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNe31rIUqVgj",
        "outputId": "63f32430-3d3e-4cf7-9afc-dcfaeb6597bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    '''\n",
        "    conv+bn+relu\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_chan, out_chan, kernel_size=1, stride=1, padding=0) -> None:\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_chan, out_chan, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(out_chan, eps=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class InceptionBlockx1(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan_pooling) -> None:\n",
        "        super().__init__()\n",
        "        self.branch1 = Conv(in_chan, 64)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            Conv(in_chan, 48, kernel_size=1),\n",
        "            Conv(48, 64, kernel_size=5, padding=2),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            Conv(in_chan, 64, kernel_size=1),\n",
        "            Conv(64, 96, kernel_size=3, padding=1),\n",
        "            Conv(96, 96, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            Conv(in_chan, out_chan_pooling, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([self.branch1(x), self.branch2(\n",
        "            x), self.branch3(x), self.branch4(x)], dim=1)\n",
        "        # print(out.shape)\n",
        "        return out\n",
        "\n",
        "\n",
        "class InceptionBlockx2(nn.Module):\n",
        "    def __init__(self, in_chan) -> None:\n",
        "        super().__init__()\n",
        "        self.branch1 = Conv(in_chan, 384, 3, 2)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            Conv(in_chan, 64, kernel_size=1),\n",
        "            Conv(64, 96, kernel_size=3, padding=1),\n",
        "            Conv(96, 96, kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        out1 = self.branch1(x)\n",
        "        out2 = self.branch2(x)\n",
        "        out3 = self.branch3(x)\n",
        "        # print(out1.shape, out2.shape, out3.shape)\n",
        "        out = torch.cat([out1, out2, out3], dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class InceptionBlockx3(nn.Module):\n",
        "    def __init__(self, in_chan, internal_chan) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1 = Conv(in_chan, 192, 1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            Conv(in_chan, internal_chan, 1),\n",
        "            Conv(internal_chan, internal_chan, [1, 7], padding=[0, 3]),\n",
        "            Conv(internal_chan, 192, [7, 1], padding=[3, 0]),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            Conv(in_chan, internal_chan, 1),\n",
        "            Conv(internal_chan, internal_chan, [7, 1], padding=[3, 0]),\n",
        "            Conv(internal_chan, internal_chan, [1, 7], padding=[0, 3]),\n",
        "            Conv(internal_chan, internal_chan, [7, 1], padding=[3, 0]),\n",
        "            Conv(internal_chan, internal_chan, [1, 7], padding=[0, 3]),\n",
        "            Conv(internal_chan, 192, [1, 7], padding=[0, 3]),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            Conv(in_chan, 192, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], dim=1)\n",
        "\n",
        "\n",
        "class InceptionBlockx4(nn.Module):\n",
        "    def __init__(self, in_chan, ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1 = nn.Sequential(\n",
        "            Conv(in_chan, 192, kernel_size=1),\n",
        "            Conv(192, 320, kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            Conv(in_chan, 192, kernel_size=1),\n",
        "            Conv(192, 192, kernel_size=[1, 7], padding=[0, 3]),\n",
        "            Conv(192, 192, kernel_size=[7, 1], padding=[3, 0]),\n",
        "            Conv(192, 192, kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n",
        "\n",
        "\n",
        "class InceptionBlockx5(nn.Module):\n",
        "    def __init__(self, in_chan) -> None:\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "            Conv(in_chan, 320, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.branch2x1 = Conv(in_chan, 384, kernel_size=1)\n",
        "        self.branch2x2 = Conv(384, 384, kernel_size=[1, 3], padding=[0, 1])\n",
        "        self.branch2x3 = Conv(384, 384, kernel_size=[3, 1], padding=[1, 0])\n",
        "\n",
        "        self.branch3x1 = Conv(in_chan, 448, kernel_size=1)\n",
        "        self.branch3x2 = Conv(448, 384, kernel_size=3, stride=1, padding=1)\n",
        "        self.branch3x3 = Conv(384, 384, kernel_size=[1, 3], padding=[0, 1])\n",
        "        self.branch3x4 = Conv(384, 384, kernel_size=[3, 1], padding=[1, 0])\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            Conv(in_chan, 192, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.branch1(x)\n",
        "        out2 = self.branch2x1(x)\n",
        "        out2 = torch.cat(\n",
        "            [self.branch2x2(out2), self.branch2x3(out2)], dim=1)\n",
        "        out3 = self.branch3x1(x)\n",
        "        out3 = self.branch3x2(out3)\n",
        "        out3 = torch.cat([self.branch3x3(out3), self.branch3x4(out3)], dim=1)\n",
        "        out4 = self.branch4(x)\n",
        "        return torch.cat([out1, out2, out3, out4], dim=1)\n",
        "\n",
        "\n",
        "class GoogleNet(nn.Module):\n",
        "    def __init__(self, nc: int) -> None:\n",
        "        super().__init__()\n",
        "        self.nc = nc\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            Conv(3, 32, kernel_size=3, stride=2),\n",
        "            Conv(32, 32, kernel_size=3),\n",
        "            Conv(32, 64, kernel_size=3, padding=1),\n",
        "        )\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            Conv(64, 80, kernel_size=1),\n",
        "            Conv(80, 192, kernel_size=3, stride=1),\n",
        "        )\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.mixer1 = nn.Sequential(\n",
        "            InceptionBlockx1(192, 32),\n",
        "            InceptionBlockx1(256, 64),\n",
        "            InceptionBlockx1(288, 64),\n",
        "        )\n",
        "\n",
        "        self.mixer2 = nn.Sequential(\n",
        "            InceptionBlockx2(288),\n",
        "        )\n",
        "        self.mixer3 = nn.Sequential(\n",
        "            InceptionBlockx3(768, 128),\n",
        "            InceptionBlockx3(768, 160),\n",
        "            InceptionBlockx3(768, 160),\n",
        "            InceptionBlockx3(768, 192),\n",
        "        )\n",
        "\n",
        "        self.mixer4 = nn.Sequential(\n",
        "            InceptionBlockx4(768),\n",
        "            InceptionBlockx5(1280),\n",
        "            InceptionBlockx5(2048),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.fc = nn.Linear(2048, self.nc)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "            stddev = float(m.stddev) if hasattr(\n",
        "                m, \"stddev\") else 0.1  # type: ignore\n",
        "            torch.nn.init.trunc_normal_(\n",
        "                m.weight, mean=0.0, std=stddev, a=-2, b=2)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.mixer1(out)\n",
        "        # print(f'out1: {out.shape}')\n",
        "        out = self.mixer2(out)\n",
        "        out = self.mixer3(out)\n",
        "        out = self.mixer4(out)\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x = torch.randn(2, 3, 224, 224)\n",
        "    net = GoogleNet(nc=1000)\n",
        "    print(net(x).shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据准备"
      ],
      "metadata": {
        "id": "08CYfsG2qvYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "# import albumentations as A\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_CIFAdataset_loader(root, batch_size, num_workers, pin_memory, valid_rate, shuffle: bool, random_seed=42, augment=True):\n",
        "    # 引入分割CIFA数据集的包，分割数据为训练集和验证集\n",
        "    from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "    # 预处理\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.ToTensor(), ]) if augment else transforms.ToTensor(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465],\n",
        "            std=[0.2023, 0.1994, 0.2010],\n",
        "        )\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.ToTensor(), ]) if augment else transforms.ToTensor(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465],\n",
        "            std=[0.2023, 0.1994, 0.2010],\n",
        "        )\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465],\n",
        "            std=[0.2023, 0.1994, 0.2010],\n",
        "        )\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    # CIFAR-10数据集\n",
        "    train_dataset = CIFAR10(root=root, train=True,\n",
        "                            download=True, transform=train_transform)\n",
        "    val_dataset = CIFAR10(root=root, train=True,\n",
        "                          download=False, transform=val_transform)\n",
        "    test_dataset = CIFAR10(root=root, train=False,\n",
        "                           download=True, transform=test_transform)\n",
        "\n",
        "    # 分割数据集\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_rate * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # 生成dataloader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=1,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader, test_loader"
      ],
      "metadata": {
        "id": "tWndWSOnqcJQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "训练\n",
        "\n",
        "    优化器：Adam\n",
        "    学习率：0.001\n",
        "    batch size： 256\n",
        "    网络：resnet34\n",
        "    数据集：CIFA-10\n",
        "    训练轮数: 10\n",
        "    accu: 0.81"
      ],
      "metadata": {
        "id": "EO6x-t4xq3sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torchmetrics\n",
        "from argparse import ArgumentParser\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def arg_parser():\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--batch-size', '-bs', type=int,\n",
        "                        default=16, required=True, help='input batch size')\n",
        "    parser.add_argument('--num-workers', '-nw',  type=int,\n",
        "                        default=4, required=True, help='number of workers')\n",
        "    # parser.add_argument('--resume', '-r', type=str,\n",
        "    #                     required=False, help='resume a train')\n",
        "    parser.add_argument('--device', type=str,\n",
        "                        help='gpu or cpu', choices=['gpu', 'cpu'], default='gpu')\n",
        "    parser.add_argument('--num-classes', '-nc', type=int,\n",
        "                        help='number of classes', required=True)\n",
        "    parser.add_argument('--lr', '-lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--epochs', type=int,\n",
        "                        required=True,  help='num of epochs')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def train_fn(net, dataloader, opt, device, criterion, writer, epoch):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    criterion.to(device)\n",
        "    for idx, (input, target) in dataloader:\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        pred = net(input)\n",
        "\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        cur_loss = train_loss/(idx+1)\n",
        "\n",
        "        dataloader.set_postfix(loss=cur_loss)\n",
        "        writer.add_scalar('training loss',\n",
        "                          cur_loss,\n",
        "                          epoch*len(dataloader)+idx)\n",
        "\n",
        "\n",
        "def val_fn(net, dataloader, device, num_classes, writer, epoch: int):\n",
        "    net.eval()\n",
        "    metric = torchmetrics.Accuracy(numClass=num_classes).to(device)\n",
        "    with torch.no_grad():\n",
        "        for idx, (input, target) in dataloader:\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            pred = net(input)\n",
        "            acc = metric.update(pred, target)\n",
        "        acc = metric.compute()\n",
        "    writer.add_scalar('val_acc', acc, epoch*len(dataloader)+idx)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(net, opt, epochs, batch_size, num_workers, device, num_classes, model='Resnet', scheduler=None):\n",
        "\n",
        "    train_dataloader, val_dataloader, _ = get_CIFAdataset_loader(\n",
        "        root='./data/CIFA', batch_size=batch_size, num_workers=num_workers, pin_memory=True, valid_rate=0.2, shuffle=True)\n",
        "\n",
        "    # 模型权重位置\n",
        "    model_path = 'runs'\n",
        "    if not os.path.exists(model_path):\n",
        "        os.mkdir(model_path)\n",
        "    save_path = os.path.join(model_path, model)\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "\n",
        "    log_dir = os.path.join(model_path,  model, 'logs')\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.mkdir(log_dir)\n",
        "\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    best = 0.0\n",
        "\n",
        "    early_stop_step = 0\n",
        "    early_stop_limit = 15\n",
        "\n",
        "    for idx in range(epochs):\n",
        "        train_loop = tqdm(enumerate(train_dataloader),\n",
        "                          total=len(train_dataloader), leave=True)\n",
        "        train_loop.set_description(f'epoch: {idx}/{epochs}')\n",
        "\n",
        "        train_fn(net=net, opt=opt,\n",
        "                 dataloader=train_loop, device=device,\n",
        "                 criterion=nn.CrossEntropyLoss(),\n",
        "                 writer=writer, epoch=idx,\n",
        "                 )\n",
        "\n",
        "        val_loop = tqdm(enumerate(val_dataloader),\n",
        "                        total=len(val_dataloader), leave=True)\n",
        "\n",
        "        score = val_fn(net=net, dataloader=val_loop,\n",
        "                       device=device, num_classes=num_classes, writer=writer, epoch=idx)\n",
        "\n",
        "        print(f'acc={score}, best acc is {max(score, best)}')\n",
        "\n",
        "        if (score > best):\n",
        "            torch.save({\n",
        "                'epoch': idx,\n",
        "                'model_state_dict': net.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "            }, os.path.join(save_path, f'epoch={idx}-miou={score:.4f}.pth'))\n",
        "            best = score\n",
        "            early_stop_step = 0\n",
        "        else:\n",
        "            if early_stop_step > early_stop_limit:\n",
        "                print(f'因为已经有{early_stop_limit}轮没有提升，训练提前终止')\n",
        "                writer.close()\n",
        "                break\n",
        "            early_stop_step += 1\n",
        "\n",
        "        if scheduler:\n",
        "            writer.add_scalar(\n",
        "                \"lr\", scheduler.get_last_lr()[-1]\n",
        "            )\n",
        "            scheduler.step()\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # args = arg_parser()\n",
        "    bs = 256  # args.batch_size\n",
        "    num_workers = 2  # args.num_workers\n",
        "    device = 'cuda'  # args.device\n",
        "    num_classes = 10  # args.num_classes\n",
        "    lr = 0.001  # args.num_classes\n",
        "    epochs = 10  # args.epochs\n",
        "\n",
        "    seed_everything(42)\n",
        "    net = GoogleNet(num_classes)\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    train(net=net, epochs=epochs, batch_size=bs, num_workers=num_workers,\n",
        "          device=device, num_classes=num_classes, opt=opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdwYiC_sq1eT",
        "outputId": "8929e44f-d2b3-44d5-861b-454959b6ec15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch: 0/10: 100%|██████████| 157/157 [05:28<00:00,  2.09s/it, loss=2.02]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc=0.41589999198913574, best acc is 0.41589999198913574\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch: 1/10: 100%|██████████| 157/157 [05:20<00:00,  2.04s/it, loss=1.53]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.73it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc=0.5206999778747559, best acc is 0.5206999778747559\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch: 2/10: 100%|██████████| 157/157 [05:19<00:00,  2.04s/it, loss=1.31]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc=0.5389999747276306, best acc is 0.5389999747276306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 3/10: 100%|██████████| 157/157 [05:20<00:00,  2.04s/it, loss=1.17]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.6118999719619751, best acc is 0.6118999719619751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 4/10: 100%|██████████| 157/157 [05:20<00:00,  2.04s/it, loss=0.991]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.6499000191688538, best acc is 0.6499000191688538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 5/10: 100%|██████████| 157/157 [05:20<00:00,  2.04s/it, loss=0.859]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.7031999826431274, best acc is 0.7031999826431274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 6/10: 100%|██████████| 157/157 [05:20<00:00,  2.04s/it, loss=0.789]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.7384999990463257, best acc is 0.7384999990463257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 7/10: 100%|██████████| 157/157 [05:20<00:00,  2.04s/it, loss=0.665]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.7491999864578247, best acc is 0.7491999864578247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 8/10: 100%|██████████| 157/157 [05:20<00:00,  2.04s/it, loss=0.614]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.7390000224113464, best acc is 0.7491999864578247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "epoch: 9/10: 100%|██████████| 157/157 [05:20<00:00,  2.04s/it, loss=0.559]\n",
            "100%|██████████| 40/40 [00:23<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.7843999862670898, best acc is 0.7843999862670898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型评估"
      ],
      "metadata": {
        "id": "OcMbIIklrOEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, dataloader, device, num_classes):\n",
        "    net.eval()\n",
        "    # metric = torchmetrics.Accuracy(numClass=num_classes).to(device)\n",
        "    correct=0.0\n",
        "    with torch.no_grad():\n",
        "        for  _, (input, target) in tqdm(enumerate(dataloader), total=len(dataloader), leave=True):\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            pred = net(input)\n",
        "            pred = F.softmax(pred, 1).argmax(1)\n",
        "            correct += pred.eq(target).sum()\n",
        "            # acc = metric.update(pred, target)\n",
        "    # acc = metric.compute()\n",
        "    print(correct / len(dataloader))\n",
        "\n",
        "_, _, test_dataloader = get_CIFAdataset_loader(\n",
        "        root='./data/CIFA', batch_size=128, num_workers=2, pin_memory=True, valid_rate=0.2, shuffle=True)\n",
        "\n",
        "test(net=net, device=device, num_classes=num_classes, dataloader=test_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIXYJXozrEBK",
        "outputId": "a33c2de6-a1a0-4b07-8e9b-03df43d8a98f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [02:58<00:00, 55.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7928, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}