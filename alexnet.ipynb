{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alexnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzZcvTpuA49A+GsIP0ltpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuzhi535/resnet-pytorch/blob/master/alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install einops torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QE7uzuB3qfo",
        "outputId": "af638dfb-efdd-4f4c-e41f-0670d2bed09c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUBJORsj3mtG",
        "outputId": "fe881b8e-dbd1-4d55-8b6e-da387b8f499c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, kernel_size=1, stride=1, padding=1) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_chan, out_chan, kernel_size=kernel_size,\n",
        "                      stride=stride, padding=padding),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "'''\n",
        "AlexNet(\n",
        "  (features): Sequential(\n",
        "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "    (1): ReLU(inplace=True)\n",
        "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "    (4): ReLU(inplace=True)\n",
        "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (7): ReLU(inplace=True)\n",
        "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (9): ReLU(inplace=True)\n",
        "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (11): ReLU(inplace=True)\n",
        "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "  )\n",
        "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
        "  (classifier): Sequential(\n",
        "    (0): Dropout(p=0.5, inplace=False)\n",
        "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
        "    (2): ReLU(inplace=True)\n",
        "    (3): Dropout(p=0.5, inplace=False)\n",
        "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
        "    (5): ReLU(inplace=True)\n",
        "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
        "  )\n",
        ")\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "class Alexnet(nn.Module):\n",
        "    '''\n",
        "    input size: batch size x 3 x 224 x 224\n",
        "    output size: batch size x num_classes\n",
        "    '''\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            Conv(3, 64, kernel_size=7, stride=4, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            Conv(64, 192, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            Conv(192, 384, kernel_size=3, padding=1),\n",
        "            Conv(384, 256, kernel_size=3, padding=1),\n",
        "            Conv(256, 256, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.dense = nn.Sequential(\n",
        "            Rearrange('b c h w -> b (c h w)'),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        # print(out.shape)\n",
        "        out = self.conv2(out)\n",
        "        # print(out.shape)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        out = self.dense(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    net = Alexnet()\n",
        "    x = torch.randn(2, 3, 224, 224)\n",
        "    out = net(x)\n",
        "    print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "# import albumentations as A\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_CIFAdataset_loader(root, batch_size, num_workers, pin_memory, valid_rate, shuffle: bool, random_seed=42, augment=True):\n",
        "    # 引入分割CIFA数据集的包，分割数据为训练集和验证集\n",
        "    from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "    # 预处理\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.ToTensor(), ]) if augment else transforms.ToTensor(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465],\n",
        "            std=[0.2023, 0.1994, 0.2010],\n",
        "        )\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.ToTensor(), ]) if augment else transforms.ToTensor(),\n",
        "                            transforms.Resize(224),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465],\n",
        "            std=[0.2023, 0.1994, 0.2010],\n",
        "        )\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465],\n",
        "            std=[0.2023, 0.1994, 0.2010],\n",
        "        )\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    # CIFAR-10数据集\n",
        "    train_dataset = CIFAR10(root=root, train=True,\n",
        "                            download=True, transform=train_transform)\n",
        "    val_dataset = CIFAR10(root=root, train=True,\n",
        "                          download=False, transform=val_transform)\n",
        "    test_dataset = CIFAR10(root=root, train=False,\n",
        "                           download=True, transform=test_transform)\n",
        "\n",
        "    # 分割数据集\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_rate * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # 生成dataloader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=1,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader, test_loader"
      ],
      "metadata": {
        "id": "Bq--Jv7T3sUf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torchmetrics\n",
        "from argparse import ArgumentParser\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def arg_parser():\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--batch-size', '-bs', type=int,\n",
        "                        default=16, required=True, help='input batch size')\n",
        "    parser.add_argument('--num-workers', '-nw',  type=int,\n",
        "                        default=4, required=True, help='number of workers')\n",
        "    # parser.add_argument('--resume', '-r', type=str,\n",
        "    #                     required=False, help='resume a train')\n",
        "    parser.add_argument('--device', type=str,\n",
        "                        help='gpu or cpu', choices=['gpu', 'cpu'], default='gpu')\n",
        "    parser.add_argument('--num-classes', '-nc', type=int,\n",
        "                        help='number of classes', required=True)\n",
        "    parser.add_argument('--lr', '-lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--epochs', type=int,\n",
        "                        required=True,  help='num of epochs')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def train_fn(net, dataloader, opt, device, criterion, writer, epoch):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    criterion.to(device)\n",
        "    for idx, (input, target) in dataloader:\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        pred = net(input)\n",
        "\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        cur_loss = train_loss/(idx+1)\n",
        "\n",
        "        dataloader.set_postfix(loss=cur_loss)\n",
        "        writer.add_scalar('training loss',\n",
        "                          cur_loss,\n",
        "                          epoch*len(dataloader)+idx)\n",
        "\n",
        "\n",
        "def val_fn(net, dataloader, device, num_classes, writer, epoch: int):\n",
        "    net.eval()\n",
        "    metric = torchmetrics.Accuracy(numClass=num_classes).to(device)\n",
        "    with torch.no_grad():\n",
        "        for idx, (input, target) in dataloader:\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            pred = net(input)\n",
        "            acc = metric.update(pred, target)\n",
        "        acc = metric.compute()\n",
        "    writer.add_scalar('val_acc', acc, epoch*len(dataloader)+idx)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(net, opt, epochs, batch_size, num_workers, device, num_classes, model='Resnet', scheduler=None):\n",
        "\n",
        "    train_dataloader, val_dataloader, _ = get_CIFAdataset_loader(\n",
        "        root='./data/CIFA', batch_size=batch_size, num_workers=num_workers, pin_memory=True, valid_rate=0.2, shuffle=True)\n",
        "\n",
        "    # 模型权重位置\n",
        "    model_path = 'runs'\n",
        "    if not os.path.exists(model_path):\n",
        "        os.mkdir(model_path)\n",
        "    save_path = os.path.join(model_path, model)\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "\n",
        "    log_dir = os.path.join(model_path,  model, 'logs')\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.mkdir(log_dir)\n",
        "\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    best = 0.0\n",
        "\n",
        "    early_stop_step = 0\n",
        "    early_stop_limit = 15\n",
        "\n",
        "    for idx in range(epochs):\n",
        "        train_loop = tqdm(enumerate(train_dataloader),\n",
        "                          total=len(train_dataloader), leave=True)\n",
        "        train_loop.set_description(f'epoch: {idx}/{epochs}')\n",
        "\n",
        "        train_fn(net=net, opt=opt,\n",
        "                 dataloader=train_loop, device=device,\n",
        "                 criterion=nn.CrossEntropyLoss(),\n",
        "                 writer=writer, epoch=idx,\n",
        "                 )\n",
        "\n",
        "        val_loop = tqdm(enumerate(val_dataloader),\n",
        "                        total=len(val_dataloader), leave=True)\n",
        "\n",
        "        score = val_fn(net=net, dataloader=val_loop,\n",
        "                       device=device, num_classes=num_classes, writer=writer, epoch=idx)\n",
        "\n",
        "        print(f'acc={score}, best acc is {max(score, best)}')\n",
        "\n",
        "        if (score > best):\n",
        "            torch.save({\n",
        "                'epoch': idx,\n",
        "                'model_state_dict': net.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "            }, os.path.join(save_path, f'epoch={idx}-miou={score:.4f}.pth'))\n",
        "            best = score\n",
        "            early_stop_step = 0\n",
        "        else:\n",
        "            if early_stop_step > early_stop_limit:\n",
        "                print(f'因为已经有{early_stop_limit}轮没有提升，训练提前终止')\n",
        "                writer.close()\n",
        "                break\n",
        "            early_stop_step += 1\n",
        "\n",
        "        if scheduler:\n",
        "            writer.add_scalar(\n",
        "                \"lr\", scheduler.get_last_lr()[-1]\n",
        "            )\n",
        "            scheduler.step()\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # args = arg_parser()\n",
        "    bs = 64  # args.batch_size\n",
        "    num_workers = 2  # args.num_workers\n",
        "    device = 'cuda'  # args.device\n",
        "    num_classes = 10  # args.num_classes\n",
        "    lr = 0.001  # args.num_classes\n",
        "    epochs = 10  # args.epochs\n",
        "\n",
        "    seed_everything(42)\n",
        "    net = Alexnet()\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    train(net=net, epochs=epochs, batch_size=bs, num_workers=num_workers,\n",
        "          device=device, num_classes=num_classes, opt=opt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAMwBmCs4Dsx",
        "outputId": "8b2f77fb-51ee-49b8-fabf-83356215d69b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 0/10: 100%|██████████| 625/625 [01:10<00:00,  8.81it/s, loss=1.92]\n",
            "100%|██████████| 157/157 [00:15<00:00, 10.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.3675000071525574, best acc is 0.3675000071525574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 1/10: 100%|██████████| 625/625 [01:25<00:00,  7.35it/s, loss=1.61]\n",
            "100%|██████████| 157/157 [00:20<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.4415000081062317, best acc is 0.4415000081062317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 2/10: 100%|██████████| 625/625 [01:28<00:00,  7.04it/s, loss=1.48]\n",
            "100%|██████████| 157/157 [00:20<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.5080999732017517, best acc is 0.5080999732017517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 3/10: 100%|██████████| 625/625 [01:28<00:00,  7.05it/s, loss=1.39]\n",
            "100%|██████████| 157/157 [00:20<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.5182999968528748, best acc is 0.5182999968528748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 4/10: 100%|██████████| 625/625 [01:09<00:00,  9.04it/s, loss=1.32]\n",
            "100%|██████████| 157/157 [00:14<00:00, 10.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.558899998664856, best acc is 0.558899998664856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 5/10: 100%|██████████| 625/625 [01:26<00:00,  7.19it/s, loss=1.25]\n",
            "100%|██████████| 157/157 [00:20<00:00,  7.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.567300021648407, best acc is 0.567300021648407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 6/10: 100%|██████████| 625/625 [01:28<00:00,  7.10it/s, loss=1.2]\n",
            "100%|██████████| 157/157 [00:21<00:00,  7.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.5995000004768372, best acc is 0.5995000004768372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 7/10: 100%|██████████| 625/625 [01:27<00:00,  7.13it/s, loss=1.15]\n",
            "100%|██████████| 157/157 [00:20<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.6069999933242798, best acc is 0.6069999933242798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 8/10: 100%|██████████| 625/625 [01:29<00:00,  7.01it/s, loss=1.12]\n",
            "100%|██████████| 157/157 [00:20<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.6177999973297119, best acc is 0.6177999973297119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 9/10: 100%|██████████| 625/625 [01:27<00:00,  7.16it/s, loss=1.09]\n",
            "100%|██████████| 157/157 [00:21<00:00,  7.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.6263999938964844, best acc is 0.6263999938964844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, dataloader, device, num_classes):\n",
        "    from torch.nn import functional as F\n",
        "    net.eval()\n",
        "    # metric = torchmetrics.Accuracy(numClass=num_classes).to(device)\n",
        "    correct=0.0\n",
        "    with torch.no_grad():\n",
        "        for  _, (input, target) in tqdm(enumerate(dataloader), total=len(dataloader), leave=True):\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            pred = net(input)\n",
        "            pred = F.softmax(pred, 1).argmax(1)\n",
        "            correct += pred.eq(target).sum()\n",
        "            # acc = metric.update(pred, target)\n",
        "    # acc = metric.compute()\n",
        "    print(correct / len(dataloader))\n",
        "\n",
        "_, _, test_dataloader = get_CIFAdataset_loader(\n",
        "        root='./data/CIFA', batch_size=128, num_workers=2, pin_memory=True, valid_rate=0.2, shuffle=True)\n",
        "\n",
        "test(net=net, device=device, num_classes=num_classes, dataloader=test_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha274yp14Hbb",
        "outputId": "29b14b4f-aab5-4ca0-eafc-8fc521f785b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:40<00:00, 250.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6505, device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}